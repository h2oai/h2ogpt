import pytest


@pytest.mark.parametrize("local_server", [True, False])
def test_readme_example(local_server):

    # self-contained example used for readme, to be copied to README_CLIENT.md if changed, setting local_server = True at first
    import os
    # The grclient.py file can be copied from h2ogpt repo and used with local gradio_client for example use
    from gradio_utils.grclient import GradioClient

    if local_server:
        client = GradioClient("http://0.0.0.0:7860")
    else:
        # if you have API key for public instance:
        client = GradioClient("https://gpt.h2o.ai", h2ogpt_key=os.getenv('H2OGPT_KEY') or os.getenv('H2OGPT_H2OGPT_KEY'))

    models = client.list_models()
    print(models)

    # LLM
    print(client.question("Who are you?", model=models[0]))

    url = "https://cdn.openai.com/papers/whisper.pdf"

    # Q/A
    print(client.query("What is whisper?", url=url, model=models[0]))
    # summarization (map_reduce over all pages if top_k_docs=-1)
    print(client.summarize("What is whisper?", url=url, top_k_docs=3, model=models[0]))
    # extraction (map per page)
    print(client.extract("Give bullet for all key points", url=url, top_k_docs=3, model=models[0]))

# This will generate something like:

# LLM:
# "Hello! I'm just an AI, my purpose is to assist and provide helpful information to the best of my abilities. I am programmed to follow ethical guidelines and promote respectful and positive interactions. I am not capable of providing harmful or offensive responses, and I will always strive to provide accurate and helpful answers to your questions. If a question does not make sense or is not factually coherent, I will do my best to explain why and provide clarification. If I don't know the answer to a question, I will not provide false information and will instead suggest ways for you to find the answer or clarify the question. Please feel free to ask me anything, and I will do my best to assist you."

# Q/A:
# Whisper is a speech processing system that is designed to work reliably without the need for dataset-specific fine-tuning to achieve high-quality results on specific distributions. It is trained on a broad and diverse distribution of audio and evaluated in a zero-shot setting, with the goal of matching human behavior much better than existing systems. Whisper models are trained on a wide set of existing speech processing datasets to check whether they can generalize well across domains, tasks, and languages. The architecture details of the Whisper model family are provided in Table 1, and the performance of Whisper models on Fleurs is provided in Table 13.

# Summarization:
# Sure! Here is a summary of the key results for Whisper:

# • Whisper is a speech recognition system that uses large-scale weak supervision to achieve robust performance across various environments.
# • Whisper achieves state-of-the-art results on English speech recognition, multilingual speech recognition, and speech translation tasks.
# • Whisper's performance on English speech recognition is close to human-level accuracy, with a WER of 3.6%.
# • Whisper's performance on multilingual speech recognition and speech translation tasks is also impressive, achieving a WER of 80.3% on 102 languages and a BLEU score of 29.1 on 21 languages, respectively.
# • Whisper's robustness to additive noise is also noteworthy, outperforming other models under intense noise conditions.
# • Whisper's long-form transcription performance is competitive with commercial ASR services, with a WER distribution that is close to human transcription performance.
# • Whisper's performance continues to increase with model size across multilingual speech recognition, speech translation, and language identification tasks, suggesting that there is still room for improvement.
# • Whisper's performance on English speech recognition, however, does not improve significantly with larger models, suggesting that it may be approaching human-level performance.
# • Whisper's performance on speech recognition tasks is robust to scaling down the dataset size, indicating that the model can generalize well to unseen data.
# • Whisper uses a combination of models to achieve high accuracy on various benchmark datasets.
# • Whisper performs well on English transcription, with a raw performance table showing high accuracy scores for various models on different datasets.
# • Whisper also performs well on non-English text, using a basic standardization process that removes unnecessary symbols and formatting to normalize text.

# Extraction:
# ['* The authors propose a novel approach to speech recognition called "Whisper" that leverages large-scale weak supervision from the internet.\n* They collect a dataset of 680,000 hours of audio with transcripts from the internet, and use a multilingual and multitask training approach.\n* They develop several automated filtering methods to improve transcript quality and remove machine-generated transcripts.\n* They use a simple data processing approach, relying on sequence-to-sequence models to learn the mapping between utterances and transcribed form.\n* They train a single model to perform the entire speech processing pipeline, including transcription, translation, voice activity detection, alignment, and language identification.\n* They use a specific format to specify all tasks and conditioning information as a sequence of input tokens to the decoder.\n* They achieve state-of-the-art results on standard benchmarks and demonstrate the effectiveness of their approach in zero-shot transfer learning settings.\n* They release inference code and models to serve as a foundation for further research on robust speech recognition.', '* The paper presents a speech recognition model called Whisper, which uses a large-scale weak supervision technique to train on a diverse set of speech processing tasks.\n* Whisper is trained on a dataset of 680k hours of speech data and is designed to work reliably without fine-tuning on specific distributions.\n* The model uses a transformer encoder-decoder architecture with a multitask training format that allows it to perform various speech processing tasks, such as speech recognition, translation, and speaker diarization.\n* Whisper models are trained with a technique called "prompting," which adds noise to the input audio to prevent the model from relying on any specific feature.\n* The authors evaluate Whisper in a zero-shot setting, where the model is tested on datasets it has not seen before, and compare its performance to human performance and supervised machine learning models.\n* Whisper models closely match human performance on several datasets and outperform supervised models on some tasks.\n* The authors suggest that the difference between human and machine performance may be due to the type of training data used, with humans being able to generalize better to out-of-distribution data.\n* The paper also discusses the issue of evaluation metrics in speech recognition, particularly the use of word error rate (WER), and proposes a text normalization method to minimize penalization of non-semantic differences.', "* The article compares the performance of a zero-shot Whisper model and a supervised LibriSpeech model on various speech recognition datasets.\n* The zero-shot Whisper model performs better than the supervised LibriSpeech model on most datasets, with an average relative error reduction of 55.2%.\n* The best zero-shot Whisper model has a word error rate (WER) of 2.5% on the LibriSpeech test-clean dataset, which is roughly the performance of modern supervised baseline or the mid-2019 state of the art.\n* The zero-shot Whisper model outperforms all benchmarked LibriSpeech models by large amounts on other datasets.\n* The article also compares the performance of the zero-shot Whisper model to prior work on multilingual speech recognition, including XLS-R, mSLAM, and Maestro.\n* The zero-shot Whisper model improves performance on Multilingual LibriSpeech but significantly underperforms prior work on VoxPopuli.\n* The article suggests that emphasizing zero-shot and out-of-distribution evaluations of models is important to avoid overstating the capabilities of machine learning systems.\n* The amount of pre-training data for a given language is only moderately predictive of the zero-shot performance of the Whisper model on that language in Fleurs.\n* The zero-shot Whisper model outperforms existing models on CoVoST2 in the overall, medium, and low-resource settings but still moderately underperforms on high-resource languages compared to prior directly supervised work.\n* The accuracy of LibriSpeech-trained models degrades faster than the best Whisper model under additive white noise and pub noise.\n* Zero-shot Whisper's accuracy at language identification is not competitive with prior supervised results on Fleurs, partially due to Whisper being heavily penalized for having no training data for 20 of Fleurs languages.", "* Whisper models achieve state-of-the-art performance in speech recognition, speech translation, and language identification tasks.\n* Whisper models are trained on a large dataset of 680,000 hours of labeled audio, which is one of the largest ever created in supervised speech recognition.\n* The performance of Whisper models improves with model size, except for English speech recognition, which shows diminishing returns due to approaching human-level performance.\n* Whisper models generalize well to out-of-distribution data, with performance close to human-level accuracy.\n* The raw dataset size is important for Whisper's performance, with smaller datasets resulting in lower performance.\n* Whisper models are robust to additive noise, with good performance under low signal-to-noise ratios.\n* Whisper models perform well on long-form transcription, with competitive performance compared to commercial ASR services.\n* Whisper's performance is close to human performance, with the computer-assisted human transcription service having the lowest aggregate WER that is 1.15% better than Whisper's.", "* Whisper achieves state-of-the-art performance on speech recognition tasks, outperforming other models by a significant margin.\n* Whisper's performance scales reliably across tasks and languages with increasing model size, indicating that larger models can achieve better performance.\n* Multitask and multilingual transfer improves with scale, and multitask models benefit more from scale than single-task models.\n* Using a text normalizer developed jointly with Whisper can reduce WERs, especially on certain datasets.\n* Strategies for reliable long-form transcription include using beam search, temperature fallback, voice activity detection, previous text conditioning, and initial timestamp constraint.\n* Adding each of these interventions incrementally reduces the WER overall, but not evenly across the dataset.\n* The heuristics used in long-form transcription serve as a workaround for the noisy predictions of the model, and further research is needed to improve the reliability of long-form decoding.", '* The article discusses the development of a speech recognition system called Whisper, which achieves state-of-the-art performance on speech recognition tasks without using self-supervised pre-training or reinforcement learning.\n* Whisper uses a large-scale weakly supervised pre-training dataset and a novel decoding strategy to improve performance.\n* The system was evaluated on several benchmarks, achieving a 30% word error rate reduction compared to previous systems.\n* The authors also studied the impact of language models on robustness and found that fine-tuning the encoder and decoder of the model improved performance.\n* They also explored the effect of adding auxiliary training objectives and found that incorporating this technique could further improve results.\n* The authors noted that scaling up weakly supervised pre-training has been underappreciated in speech recognition research and demonstrated that simply training on a large and diverse supervised dataset can significantly improve robustness.\n* They also identified the need for more data for lower-resource languages as an area for future work.\n* The study highlights the potential of large-scale weakly supervised pre-training for speech recognition systems, and the importance of focusing on zero-shot transfer performance.', "* Chen et al. (2022a) proposed UniSpeech-sat, a speaker-aware pre-training method for speech representation learning that achieved state-of-the-art performance on multiple benchmark datasets.\n* Harris et al. (2020) introduced Array programming with NumPy, a library for efficient numerical computing in Python.\n* Chen et al. (2016) proposed a training method for deep neural networks with sublinear memory cost.\n* Kuchaiev et al. (2017) introduced Gaussian Error Linear Units (GELU), a widely used activation function in deep learning.\n* Hendrycks and Gimpel (2016) proposed a method for training deep neural networks with adversarial examples.\n* Lake et al. (2017) developed a method for building machines that learn and think like people.\n* Hsu et al. (2021a) proposed Hubert, a self-supervised speech representation learning model that achieved state-of-the-art performance on multiple benchmark datasets.\n* Liao et al. (2013) introduced a large-scale deep neural network acoustic modeling method for YouTube video transcription.\n* Likhomanenko et al. (2020) analyzed the domain shift in self-supervised pre-training for speech recognition.\n* Loshchilov and Hutter (2017) proposed a decoupled weight decay regularization method for deep neural networks.\n* Luong et al. (2015) introduced a multi-task sequence-to-sequence learning model.\n* Mahajan et al. (2018) explored the limits of weakly supervised pre-training for speech recognition.\n* Jia and Liang (2017) proposed an adversarial example method for evaluating reading comprehension systems.\n* Johnson et al. (2017) developed Google's multilingual neural machine translation system.\n* Kendall and Farrington (2021) created the Corpus of Regional African American Language, a resource for studying language variation and change.\n* Meyer et al. (2020) introduced the Artie Bias Corpus, an open dataset for detecting demographic bias in speech applications.\n* Koenecke et al. (2020) investigated racial disparities in automated speech recognition.\n* Kolesnikov et al. (2020) proposed Big Transfer (", '* Ravanelli et al. (2021) introduced SpeechBrain, a general-purpose speech toolkit that provides various speech processing functionalities.\n* Recht et al. (2020) showed that imageNet classifiers generalize well to ImageNet, but their performance degrades when transferred to other datasets.\n* Polyak and Juditsky (1992) proposed a method for accelerating stochastic approximation by averaging, which was later applied in speech recognition.\n* Pratap et al. (2020) introduced Massively Multilingual ASR, a large-scale multilingual dataset for speech research.\n* Sennrich et al. (2015) proposed a neural machine translation model using subword units for rare words.\n* Wang et al. (2020) introduced Voxpopuli, a large-scale multilingual speech corpus for representation learning, semi-supervised learning, and interpretation.\n* Taori et al. (2020) measured the robustness of natural distribution shifts in image classification.\n* Xu et al. (2020) proposed a multitask training method with text data for end-to-end speech recognition.\n* Watanabe et al. (2020) tackled multispeaker speech recognition for unsegmented recordings with the Chime-6 challenge.\n* Toshniwal et al. (2018) proposed a single end-to-end model for multilingual speech recognition.\n* Zhang et al. (2020) pushed the limits of semi-supervised learning for automatic speech recognition.\n* Vaswani et al. (2017) introduced attention is all you need, a neural network architecture for sequence-to-sequence tasks.\n* Virtanen et al. (2020) developed SciPy 1.0, a fundamental algorithm for scientific computing in Python.\n* Wang et al. (2020) proposed Fairseq S2T, a fast speech-to-text modeling method.\n* The authors evaluated various speech recognition systems on short-form and long-form English', '* The paper compares the performance of various speech recognition models on different datasets, including English and non-English languages.\n* The authors use a variety of datasets, including LibriSpeech, Common Voice, and their own dataset, CORAAL.\n* They also use different versions of the Whisper model, as well as other models such as wav2vec2 and Hubert.\n* The authors employ a text standardization technique to normalize English texts in different styles.\n* The raw performance tables show the word error rates (WERs) for different models on different datasets.\n* The best-performing model overall is the wav2vec2-large-960h-lv60-self model, with a WER of 5.6% on the LibriSpeech test set.\n* The best-performing model on the CORAAL dataset is the wav2vec2-large-960h model, with a WER of 11.7%.\n* The authors note that their text standardization approach is imperfect and may produce unexpected results.\n* They also mention that all the models they use are entirely or partially trained on LibriSpeech.', '* The table shows the English transcription WER (%) with greedy decoding and beam search with temperature fallback for various models.\n* The best performing model for English transcription is Whisper large-v2, with a WER of 4.3% using greedy decoding and 3.7% using beam search with temperature fallback.\n* For multilingual transcription, the best performing model is Whisper large-v2, with a WER of 10.3% using greedy decoding and 8.5% using beam search with temperature fallback.\n* The table also shows the WER (%) on MLS (Multilingual LibriSpeech) for various models.\n* The best performing model on MLS is Whisper large-v2, with a WER of 16.0% using greedy decoding and 13.5% using beam search with temperature fallback.\n* The table also includes results for other models, such as Whisper tiny, Whisper base, Whisper small, and Whisper medium, as well as results for different languages.', '* The table shows the WER (%) on CommonVoice9 and VoxPopuli for various language models.\n* The best performing model for CommonVoice9 is Whisper large-v2, with a WER of 3.1%.\n* The best performing model for VoxPopuli is Whisper large-v2, with a WER of 12.6%.\n* The WERs for both datasets decrease as the size of the model increases.\n* The WERs for both datasets are generally lower for the Whisper models than for the other models.\n* There are some exceptions to this trend, such as the WER for the German model on VoxPopuli, which is higher for the Whisper models than for the other models.\n* It appears that the Whisper models are more effective at recognizing speech in both CommonVoice9 and VoxPopuli than the other models.', '* The table displays the WER (%) on Fleurs for various languages and models.\n* The best-performing model for each language is marked in bold.\n* For some languages, there are multiple models with the same name but different capitalization or accents; these are not necessarily the same model.\n* The table includes languages such as Azerbaijani, Belarusian, Bulgarian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Greek, Hebrew, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Kazakh, Korean, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malay, Maltese, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Spanish, Swahili, Swedish, Tajik, Thai, Turkish, Uzbek, Vietnamese, Welsh, and Yoruba.\n* The table also includes some language variants, such as Arabic, Assamese, Bengali, Gujarati, Haryanvi, Javanese, Kannada, Malayalam, Marathi, Nepali, Odia, Punjabi, Sindhi, Tamil, Telugu, Urdu, and Vietnamese.\n* The Whisper model is used for most languages, with different variations such as Whisper tiny, Whisper base, Whisper small, Whisper medium, Whisper large, and Whisper large-v2.\n* The table shows the WER (%) for each language and model, with lower values indicating better performance.\n* Some languages have multiple models with different WERs, indicating that there are variations in performance for that language.\n* The table does not provide any information about the data used to train the models or the evaluation metrics used to calculate the WER.\n* The table does not provide any information about the hardware or software used to run the models.\n* The table does not provide any information about the research question or purpose of the study.\n* The table does not provide any information about the methodology or approach used to develop the models.\n* The table does not provide any information about the limitations or potential biases of the study.\n* The table does not provide any information about the implications or applications of the study.\n* The table does not provide any information about the future work or directions for research.\n* The table does not provide any information about the funding or support for the study.\n* The table does not provide any information about the conflicts of interest or ethical considerations.\n* The table does not provide any information about the author(s) or their affiliations.\n* The table does not provide any information about the publication or conference where the study was published.\n* The table does not provide any information about the date or timeline of the study.\n* The table does not provide any information about the dataset used for the study.\n* The table does not provide any information about the evaluation metric used to calculate the WER.\n* The table does not provide any information about the statistical significance of the results.\n* The table does not provide any information about the sample size or population used in the study.\n* The table does not provide any information about the experimental design or methodology used in the study.\n* The table does not provide any information about the control group or baseline used in the study.\n* The table does not provide any information about the variables or features used in the study.\n* The table does not provide any information about the machine learning algorithms or techniques used in the study.\n* The table does not provide any information about the hyperparameters or parameters used in the study.\n* The table does not provide any information about the training or validation process used in the study.\n* The table does not provide any information about the', 'Sure, here is a summary of the key results based on the provided document:\n\n• The paper presents a study on robust speech recognition via large-scale weak supervision.\n• The study uses a dataset of 1000 hours of speech from 20 languages, with 1000 speakers, and 1000000 words.\n• The authors use a transformer-based model, called Whisper, which achieves state-of-the-art results on LibriSpeech and Fleurs.\n• Whisper has five variants: tiny, base, small, medium, and large.\n• The study compares the performance of Whisper variants on both datasets and shows that larger models perform better.\n• The authors also experiment with different combinations of data augmentation techniques and show that they improve performance.\n• They also test their model on CoVoST2, a long-form transcription task, and achieve good results.\n• Finally, they analyze the performance of their model on different languages and show that it performs well on most of them.', 'Sure! Here is a summary of the key results from the given document:\n\n• The authors achieved robust speech recognition via large-scale weak supervision.\n• They used a dataset with 100 hours of audio for 100 languages.\n• They trained a model with 1048576 parameters, a batch size of 256, and 2048 warmup updates.\n• They used AdamW optimizer with β1 = 0.9, β2 = 0.98, ϵ = 10−6, and weight decay of 0.1.\n• They used a learning rate schedule with a speechless audio subsample factor of 10× and a condition on prior text rate.\n• They changed the hyperparameters for Whisper Large V2, including the batch size, BPE dropout, and stochastic depth.\n• They used different learning rates for different models, including Tiny Base, Small, Medium, Large, and Large V2.\n\nI hope this helps! Let me know if you have any further questions.']
